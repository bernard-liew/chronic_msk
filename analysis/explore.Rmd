---
title: "explore"
author: "Bernard"
date: "2022-10-06"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Load package

```{r}
library(tidyverse)
library(data.table)
library (cowplot)
library (officer)
library (flextable)
library (dataPreparation)

# ML
library (mgcv)
library (mgcViz)
library (buildmer)
library (DHARMa)
library (gratia)
```

## Import data

```{r}
df <- rio::import ("data/BBDD.xlsx")
df <- janitor::clean_names(df)
```

# Explore

```{r}
skimr::skim (df)

mice::md.pattern(df, rotate.names = TRUE)
```


# Subset

```{r}
df2 <- df %>%
  dplyr::select(gender, 
         age, 
         main_pain_location, 
         pain_extent,
         profession_status, 
         educational_level, 
         comorbidity,
         gcps_total_baseline,
         expect_final_score,
         lot_r_final_score,
         cpaq_final_score,
         gcps_total_1year_follow_up) 

names(df2) <- c("gender",
                "age",
                "pain_loc",
                "pain_ext",
                "work",
                "education",
                "comorbidity",
                "gcps",
                "expect",
                "lot",
                "cpaq",
                "gcps_t1")

df2 <- df2%>%
  mutate (pain_loc = case_when(
    
    pain_loc %in% c(2, 7, 8, 9) ~ "UL",
    pain_loc %in% c(3, 4, 5) ~ "LL",
    pain_loc == 0 ~ "Lx",
    pain_loc == 1 ~ "Cx",
    TRUE ~ "FM"
  )) %>%
  mutate_at (c("gender", "pain_loc", "pain_ext", 
               "work", "education", "comorbidity"), factor)
skimr::skim (df2)
```

# Impute

```{r}
# imp <- mice::mice(df2, m = 20, seed = 155, maxit = 30, meth = "rf")
# 
# saveRDS(imp, "output/imp.RDS")
imp <- readRDS("output/imp.RDS")

set.seed(123)
rand_n <- sample (1:20, 1)

df3 <- mice::complete (imp, rand_n)

pred_num <- c("age", "gcps", "expect", "lot", "cpaq")

skimr::skim (df3)

df4 <- df3 %>%
  mutate_at(vars (age, gcps, expect, lot, cpaq), scale, center = TRUE, scale = FALSE)

```

```{r}
ggplot (df3) +
  geom_point(aes (x = cpaq, y = gcps_t1))
```

# GAM

## Formula

```{r}
fgam <- gcps_t1 ~ 
  gender +
  pain_loc  + 
  pain_ext  + 
  work  + 
  comorbidity  + 
  s(age)  + 
  s(gcps)  + 
  s(expect)  + 
  s(lot)  + 
  s(cpaq) 

```


### Gaussian distribution

```{r}

gam1 <- gam(fgam, data = df4)
summary(gam1) # deviance explained = 42.4%
anova(gam1)

appraise(gam1)

simulationOutput <- simulateResiduals(fittedModel = gam1)
plot(simulationOutput)
testZeroInflation(simulationOutput)
```

### ZiP distribution

```{r}
gam2 <- gam(fgam, data = df4, family = ziP)
summary(gam2) # deviance explained = 94.6%
anova(gam2)

appraise(gam2)

b <- getViz(gam2)
print(plot(b, allTerms = T), pages = 1) 

p <- tidygam::predict_gam(gam2, 
                          series = "gcps",
                          exclude_terms = list("gender",
                                                   "pain_loc",
                                                   "pain_ext",
                                                   "work",
                                                   "comorbidity", 
                                                   "s(age)",
                                                   "s(expect)",
                                                   "s(lot)",
                                                   "s(cpaq)"))

simulationOutput <- simulateResiduals(fittedModel = gam2)
plot(simulationOutput)
testZeroInflation(simulationOutput)

gam.check(gam2)
# convergence check
gam2$outer.info
# examine if the range of predicted values 
# is sane for the zero cases
range(predict(gam2, type = "response")[df4$gcps_t1==0])
# check zero prediction 
boxplot(predict(gam2) ~ (df4$gcps_t1==0))

f = tidymv::plot_smooths(model = gam2, series = gcps)
```

## Stepwise

```{r}
gam3 <- buildgam(fgam, data = df4, family = ziP)
b <- getViz(gam3@model)
print(plot(b, allTerms = T), pages = 1) 

draw(gam3@model, select = c("s(cpaq)", "s(expect)", "s(age)"))
```

## Analyse zero-inflated normal

```{r}
# remotes::install_github("neural-structured-additive-learning/mixdistreg")
library(mixdistreg)
# zero-inflated normal instead of ziP
zinorm <- zinreg(
  list_of_formulas = list(mean = ~ gender +
                            pain_loc  + 
                            pain_ext  + 
                            work  + 
                            comorbidity  + 
                            s(age)  + 
                            s(gcps)  + 
                            s(expect)  + 
                            s(lot)  + 
                            s(cpaq),
                          scale = ~ 1),
  formula_inflation = ~ 1,
  y = df4$gcps_t1,
  data = df4,
  optimizer = optimizer_rmsprop(learning_rate = 0.01),
  penalty_options = penalty_control(df = 12)
)

zinorm %>% fit(epochs = 1000L, early_stopping = TRUE,
               patience = 50L, validation_split = 0.1,
               verbose = TRUE)

par(mfrow = c(2,3))
# smoothing different than in gam as df are fixed prior to model fitting
zinorm %>% plot(which_dist = "normal")
# categorical effects are pretty large -> not sure if
# everything is correct
zinorm %>% coef(which_dist = "normal")

par(mfrow=c(1,2))
# check "residuals" vs. fitted
plot(df4$gcps_t1 - fitted(zinorm) ~ fitted(zinorm))
points(df4$gcps_t1 - predict(gam2, type = "response") ~ predict(gam2, type = "response"), col = "red")
# check fitted vs. truth
plot(fitted(zinorm) ~ df4$gcps_t1)
points(predict(gam2, type = "response") ~ df4$gcps_t1, col = "red")

# in-sample mse
Metrics::mse(df4$gcps_t1, predict(gam2, type = "response"))
Metrics::mse(df4$gcps_t1, predict(zinorm))
# gam (and/or ziP) better in-sample (does not say something about out-of-sample)

# => check why:
ziP_nn <- zinreg(
  list_of_formulas = list(mean = ~ gender +
                            pain_loc  + 
                            pain_ext  + 
                            work  + 
                            comorbidity  + 
                            s(age)  + 
                            s(gcps)  + 
                            s(expect)  + 
                            s(lot)  + 
                            s(cpaq)),
  formula_inflation = ~ 1,
  family= "poisson",
  y = df4$gcps_t1,
  data = df4,
  optimizer = optimizer_rmsprop(learning_rate = 0.01),
  penalty_options = penalty_control(df = 12)
)

ziP_nn %>% fit(epochs = 1000L, early_stopping = TRUE,
               patience = 50L, validation_split = 0.1,
               verbose = TRUE)

Metrics::mse(df4$gcps_t1, predict(ziP_nn))
# => so gam fits the data better, but for given
# smoothness (as in mixdistreg), ziP is better than
# zinormal
```

# Example of a conditional zero-inflation

```{r}
ziPcond <- zinreg(
  list_of_formulas = list(mean = ~ gender +
                            pain_loc  + 
                            pain_ext  + 
                            work  + 
                            comorbidity  + 
                            s(age)  + 
                            s(gcps)  + 
                            s(expect)  + 
                            s(lot)  + 
                            s(cpaq)),
  family = "poisson",
  formula_inflation = ~ 1 + s(age), 
  # -> now probability being zero
  # is modelled to depend on the age as well
  y = df4$gcps_t1,
  data = df4,
  optimizer = optimizer_rmsprop(learning_rate = 0.01),
  penalty_options = penalty_control(df = 12)
)

# much more difficult optimization problem as it seems
ziPcond %>% fit(epochs = 1000L, early_stopping = TRUE,
                patience = 150L, validation_split = 0.1,
                verbose = TRUE)

par(mfrow = c(2,3))
# smoothing different than in gam as df are fixed prior to model fitting
ziPcond %>% plot(which_dist = "poisson")
# smooths for age for the probability to be of one or the other
# class (zero / no-zero)
par(mfrow = c(1,2))
ziPcond %>% plot() # not much different except for young age

# now check estimated effect 
ziPcond %>% coef(which_dist = "poisson")

par(mfrow=c(1,2))
# check "residuals" vs. fitted
plot(df4$gcps_t1 - fitted(ziPcond) ~ fitted(ziPcond))
points(df4$gcps_t1 - predict(gam2, type = "response") ~ predict(gam2, type = "response"), col = "red")
# check fitted vs. truth
plot(fitted(ziPcond) ~ df4$gcps_t1)
points(predict(gam2, type = "response") ~ df4$gcps_t1, col = "red")

# in-sample mse
Metrics::mse(df4$gcps_t1, predict(gam2, type = "response"))
Metrics::mse(df4$gcps_t1, predict(ziPcond))
# gam again better (conditioning makes the model worse)
```

